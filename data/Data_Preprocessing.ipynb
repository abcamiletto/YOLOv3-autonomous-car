{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing main packages\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "The main problem is that a 1.5GB JSON file is pretty much untractable, so i decided to split in one JSON per each image, deleting all useless information like lines and drivable areas.\n",
    "\n",
    "Other than that, it seems like there are 140 images w/o label. That's pretty strange since i didn't see anyone ever mentioning it on the forums, but for now i will delete them and act like they never existed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "main_folder = str(pathlib.Path.cwd().joinpath('dataset_bdd', 'labels'))\n",
    "\n",
    "json_train_dir = main_folder + '/bdd100k_labels_images_train.json'\n",
    "json_val_dir = main_folder + '/bdd100k_labels_images_validation.json'\n",
    "\n",
    "folder_train = main_folder + '/train_jsons/'\n",
    "folder_val = main_folder + '/val_jsons/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into different single JSONs\n",
    "And using more useful coordinates to describe the bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#A function to get more meaningful values to describe the bounding boxes\n",
    "def pov_change(x1,x2,y1,y2):\n",
    "    xb = (x1+x2)/(2)\n",
    "    yb = (y1+y2)/(2)\n",
    "    wb = abs(x1-x2)/2\n",
    "    hb = abs(y1-y2)/2\n",
    "    return xb,yb,wb,hb\n",
    "\n",
    "#Loading a JSON file into a Python variable\n",
    "def json_parser(path):\n",
    "    # Parsing\n",
    "    with open(path, 'r') as read_file:\n",
    "        data = json.load(read_file)\n",
    "    return data\n",
    "\n",
    "#Cleaning a JSON file from all the useless information (in respect to our task)\n",
    "def json_cleaner(data):\n",
    "    # Cleaning timestamps and not so useful attributes\n",
    "    for item in data:\n",
    "        del item['attributes']\n",
    "        del item['timestamp']\n",
    "\n",
    "    # Cleaning drivable area and lanes\n",
    "    for item in data:\n",
    "        storing_indexes = []\n",
    "        for index, i in enumerate(item['labels']):\n",
    "            del i['attributes']\n",
    "            del i['manualShape']\n",
    "            del i['manualAttributes']\n",
    "            if 'box2d' in i:\n",
    "                xb,yb,wb,hb = pov_change(i['box2d']['x1'],i['box2d']['x2'],i['box2d']['y1'],i['box2d']['y2'])\n",
    "                i['box2d']['xb'] = round(xb,2)\n",
    "                i['box2d']['yb'] = round(yb,2)\n",
    "                i['box2d']['wb'] = round(wb,2)\n",
    "                i['box2d']['hb'] = round(hb,2)\n",
    "                del i['box2d']['x1']\n",
    "                del i['box2d']['x2']\n",
    "                del i['box2d']['y1']\n",
    "                del i['box2d']['y2']\n",
    "            # Checking if anything is corrupted\n",
    "            if not 'poly2d' in i and not 'box2d' in i: print('no box2d?' + str(i['id']))\n",
    "            if 'box3d' in i: print('wtf')\n",
    "                \n",
    "            del i['id']\n",
    "            if i['category'] == 'lane' or i['category'] == 'drivable area':\n",
    "                storing_indexes.append(index)\n",
    "        storing_indexes.sort(reverse=True)\n",
    "        for indexes in storing_indexes:\n",
    "            del item['labels'][indexes]\n",
    "    return data\n",
    "\n",
    "#Dividing a single JSON into multiple ones\n",
    "def split_data(data,path):\n",
    "    for item in data:\n",
    "        name = item['name']\n",
    "        with open(path + name + '.json', 'w') as file_to_write:\n",
    "            json.dump(item, file_to_write, indent = 4)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Down below you can see how ugly formatted was the main JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Setup of the dataset:\n",
      "Data type: <class 'list'>\n",
      "Element of the list: <class 'dict'>\n",
      "Keys of the dictionaries: \n",
      "    name\n",
      "    attributes\n",
      "    timestamp\n",
      "    labels\n",
      "Dict example:\n",
      "    Key: name\n",
      "      Value: b1c66a42-6f7d68ca.jpg\n",
      "    Key: attributes\n",
      "      Value: {'weather': 'overcast', 'scene': 'city street', 'timeofday': 'daytime'}\n",
      "    Key: timestamp\n",
      "      Value: 10000\n",
      "    Key: labels\n",
      "      Value: it is a <class 'list'> made of <class 'dict'>\n",
      "{'category': 'traffic sign', 'attributes': {'occluded': False, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 1000.698742, 'y1': 281.992415, 'x2': 1040.626872, 'y2': 326.91156}, 'id': 0}\n",
      "{'category': 'traffic sign', 'attributes': {'occluded': False, 'truncated': False, 'trafficLightColor': 'none'}, 'manualShape': True, 'manualAttributes': True, 'box2d': {'x1': 214.613695, 'y1': 172.190058, 'x2': 274.505889, 'y2': 229.586743}, 'id': 1}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Down here we have a full example of a cleaned single JSON \n",
      "\n",
      "{'name': 'b1c66a42-6f7d68ca.jpg', 'labels': [{'category': 'traffic sign', 'box2d': {'xb': 1020.66, 'yb': 304.45, 'wb': 19.96, 'hb': 22.46}}, {'category': 'traffic sign', 'box2d': {'xb': 244.56, 'yb': 200.89, 'wb': 29.95, 'hb': 28.7}}, {'category': 'traffic sign', 'box2d': {'xb': 813.54, 'yb': 327.54, 'wb': 16.22, 'hb': 14.35}}, {'category': 'traffic sign', 'box2d': {'xb': 668.8, 'yb': 309.44, 'wb': 16.22, 'hb': 6.24}}, {'category': 'traffic light', 'box2d': {'xb': 711.84, 'yb': 320.05, 'wb': 4.37, 'hb': 8.11}}, {'category': 'traffic light', 'box2d': {'xb': 631.36, 'yb': 306.32, 'wb': 4.99, 'hb': 10.61}}, {'category': 'traffic light', 'box2d': {'xb': 323.17, 'yb': 298.21, 'wb': 6.24, 'hb': 8.73}}, {'category': 'traffic sign', 'box2d': {'xb': 290.1, 'yb': 295.09, 'wb': 19.34, 'hb': 6.86}}, {'category': 'traffic sign', 'box2d': {'xb': 226.47, 'yb': 306.32, 'wb': 5.61, 'hb': 5.61}}, {'category': 'car', 'box2d': {'xb': 243.94, 'yb': 363.1, 'wb': 38.06, 'hb': 24.96}}, {'category': 'car', 'box2d': {'xb': 88.59, 'yb': 372.45, 'wb': 41.18, 'hb': 28.07}}, {'category': 'car', 'box2d': {'xb': 296.97, 'yb': 369.96, 'wb': 49.91, 'hb': 25.58}}, {'category': 'car', 'box2d': {'xb': 26.2, 'yb': 369.96, 'wb': 26.2, 'hb': 33.07}}, {'category': 'rider', 'box2d': {'xb': 656.94, 'yb': 368.09, 'wb': 6.86, 'hb': 13.73}}, {'category': 'motor', 'box2d': {'xb': 656.32, 'yb': 377.45, 'wb': 7.49, 'hb': 10.61}}, {'category': 'traffic light', 'box2d': {'xb': 719.33, 'yb': 345.0, 'wb': 6.86, 'hb': 8.11}}, {'category': 'car', 'box2d': {'xb': 702.49, 'yb': 374.95, 'wb': 18.72, 'hb': 18.09}}, {'category': 'car', 'box2d': {'xb': 719.95, 'yb': 377.45, 'wb': 13.73, 'hb': 13.1}}, {'category': 'car', 'box2d': {'xb': 744.29, 'yb': 383.06, 'wb': 16.84, 'hb': 17.47}}, {'category': 'car', 'box2d': {'xb': 778.6, 'yb': 385.56, 'wb': 29.95, 'hb': 23.71}}, {'category': 'car', 'box2d': {'xb': 846.6, 'yb': 391.79, 'wb': 59.27, 'hb': 33.69}}, {'category': 'car', 'box2d': {'xb': 919.59, 'yb': 413.63, 'wb': 39.93, 'hb': 38.06}}, {'category': 'car', 'box2d': {'xb': 1073.69, 'yb': 409.89, 'wb': 137.88, 'hb': 74.24}}, {'category': 'car', 'box2d': {'xb': 1241.52, 'yb': 472.9, 'wb': 37.43, 'hb': 57.4}}, {'category': 'traffic light', 'box2d': {'xb': 606.41, 'yb': 346.88, 'wb': 3.74, 'hb': 6.24}}, {'category': 'traffic light', 'box2d': {'xb': 594.55, 'yb': 335.02, 'wb': 3.12, 'hb': 6.86}}, {'category': 'traffic light', 'box2d': {'xb': 532.79, 'yb': 336.89, 'wb': 3.74, 'hb': 4.99}}, {'category': 'car', 'box2d': {'xb': 558.37, 'yb': 361.85, 'wb': 6.86, 'hb': 6.24}}, {'category': 'car', 'box2d': {'xb': 580.21, 'yb': 360.6, 'wb': 6.24, 'hb': 8.73}}, {'category': 'car', 'box2d': {'xb': 589.56, 'yb': 367.46, 'wb': 8.11, 'hb': 10.61}}, {'category': 'car', 'box2d': {'xb': 616.39, 'yb': 368.71, 'wb': 19.96, 'hb': 16.84}}, {'category': 'car', 'box2d': {'xb': 526.55, 'yb': 361.85, 'wb': 8.73, 'hb': 6.24}}, {'category': 'car', 'box2d': {'xb': 509.71, 'yb': 363.72, 'wb': 8.11, 'hb': 6.86}}, {'category': 'car', 'box2d': {'xb': 466.04, 'yb': 363.1, 'wb': 13.1, 'hb': 9.98}}]}\n"
     ]
    }
   ],
   "source": [
    "#Example function to show the difference between the data before and after the cleaning\n",
    "def showme_data_format(path):\n",
    "    with open(path, 'r') as read_file:\n",
    "        data = json.load(read_file)\n",
    "    print('Data type: ' + str(type(data)))\n",
    "    print('Element of the list: ' + str(type(data[0])))\n",
    "    print('Keys of the dictionaries: ')\n",
    "    for key in data[0]:\n",
    "        print('    ' + str(key))\n",
    "    print('Dict example:')\n",
    "    for key, value in data[0].items():\n",
    "        if key != 'labels':\n",
    "            print('    Key: ' + str(key))\n",
    "            print('      Value: ' + str(value))\n",
    "        else:\n",
    "            print('    Key: ' + str(key))\n",
    "            print('      Value: it is a ' + str(type(value)) + ' made of ' + str(type(value[0])))\n",
    "            for ondex, obj in enumerate(value):\n",
    "                if ondex < 2: print(obj)\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "print('Initial Setup of the dataset:')\n",
    "showme_data_format(main_folder + '/try.json')\n",
    "data = json_parser(main_folder + '/try.json')\n",
    "data_cleaned = json_cleaner(data)\n",
    "print('Down here we have a full example of a cleaned single JSON \\n')\n",
    "print(data_cleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the actual code that splits the original file: be aware that if you run it it may take really long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = False\n",
    "if runtime:\n",
    "    data_train = json_parser(json_train_dir)\n",
    "    data_train = json_cleaner(data_train)\n",
    "    split_data(data_train, folder_train)\n",
    "    print('Training data: done')\n",
    "    data_val = json_parser(json_val_dir)\n",
    "    data_val = json_cleaner(data_val)\n",
    "    split_data(data_val, folder_val)\n",
    "    print('Validation data: done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting corrupted image\n",
    "Making sure every image in the train folder has a json file attached to it, otherwise delete the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images w/o label:138\n"
     ]
    }
   ],
   "source": [
    "def detect_missing_labels(delete = False):\n",
    "    counter = 1\n",
    "    main_dir = pathlib.Path.cwd().joinpath('dataset_bdd')\n",
    "    img_dir = main_dir.joinpath('images', '100k', 'train').glob('*.jpg')\n",
    "    label_dir = main_dir.joinpath('labels', 'train_jsons')\n",
    "    for img in img_dir:\n",
    "        img_name = img.name\n",
    "        label_path = label_dir.joinpath(img_name + '.json')\n",
    "        if not label_path.is_file():\n",
    "            counter = counter + 1\n",
    "            if delete:\n",
    "                img.unlink()\n",
    "                print('deleted')\n",
    "    print('Images w/o label:' + str(counter))\n",
    "    \n",
    "detect_missing_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images w/o label:1\n"
     ]
    }
   ],
   "source": [
    "detect_missing_labels(delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 720\n",
    "img_width = 1280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here below a function to convert a file path to a tuple with both image and labels\n",
    "\n",
    "data_dir = str(pathlib.Path.cwd().joinpath('dataset_bdd', 'images', '100k'))\n",
    "pathlist_img = pathlib.Path(data_dir).glob('train/*.jpg')\n",
    "\n",
    "def data_encoding(data): #Convert the class labels given as a dict into one-hot vector encoding, and concatenate it with the bounding box coordinates.\n",
    "    #It return a Tensorflow Tensor\n",
    "    labels = tf.constant([])\n",
    "    class_names = ['bus', 'traffic light', 'traffic sign', 'person', 'bike', 'truck', 'motor', 'car', 'train', 'rider']\n",
    "    for index,obj in enumerate(data['labels']):\n",
    "        label_class = class_names.index(obj['category'])\n",
    "        temp1 = tf.one_hot(label_class, len(class_names))\n",
    "        temp2 = tf.constant([x for x in obj['box2d'].values()])\n",
    "        temp3 = tf.concat([temp2, temp1], 0)\n",
    "        labels = tf.concat([labels,temp3], 0)\n",
    "    labels = tf.reshape(labels, [-1,14])\n",
    "    return labels\n",
    "\n",
    "def get_label(image_path): #Given the image path, it loads the labels from the JSON\n",
    "    if isinstance(image_path, str):\n",
    "        image_path = pathlib.Path(image_path)\n",
    "    file_name = image_path.name + '.json'\n",
    "    label_path = image_path.parent.parent.parent.parent.joinpath('labels', 'train_jsons', file_name)\n",
    "    with open(label_path, 'r') as read_file:\n",
    "        data = json.load(read_file)\n",
    "    labels = data_encoding(data)\n",
    "    return labels\n",
    "\n",
    "def process_path(file_path): #Given an image path, it return a tuple (img, labels) where both are tensors\n",
    "    labels = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    filepath = tf.constant(str(file_path))\n",
    "    img = tf.io.read_file(filepath)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img, labels\n",
    "\n",
    "\n",
    "def dataset_generator():\n",
    "    for path in pathlist_img:\n",
    "        img, labels = process_path(path)\n",
    "        yield img, labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(dataset_generator, (tf.uint8, tf.float32) , (tf.TensorShape([720, 1280, 3]), tf.TensorShape([None, 14])))\n",
    "print(dataset)\n",
    "\n",
    "dataset_s = dataset.shuffle(70000, reshuffle_each_iteration=False)\n",
    "print(dataset)\n",
    "\n",
    "val_size = int(7000)\n",
    "train_ds = dataset_s.skip(val_size)\n",
    "val_ds = dataset_s.take(val_size)\n",
    "\n",
    "print(train_ds)\n",
    "print(val_ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
