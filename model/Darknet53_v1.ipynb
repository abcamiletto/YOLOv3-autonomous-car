{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backbone Feature Detector - Darknet53\n",
    "This will be an implementation of the backbonepart of Yolov3. The various comment will guide you into the code, but for a more general understanding of the project i suggest to read the README.md\n",
    "\n",
    "## Importing the main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility libraries\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Add, BatchNormalization, GlobalAveragePooling2D, Dense, Softmax, Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension (for visualization purposes)\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model designing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Blocks\n",
    "First of all, we need to write down the code for the residual blocks used in the Darknet-53, which is a 53 layer deep feature extractor used as a backbone for YOLOv3.\n",
    "\n",
    "![Darknet_Architecture](img/darknet5353.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2D_plus(inputs, filters, kernel_size, stride = 1) -> Tensor:\n",
    "    X = Conv2D(filters = filters,\n",
    "                   kernel_size = kernel_size,\n",
    "                   strides = stride,\n",
    "                   padding = \"same\",\n",
    "                   use_bias = False)(inputs)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = LeakyReLU(alpha=0.1)(X)\n",
    "    return X\n",
    "\n",
    "def ResidualUnit(inputs, filters_alpha, filters_beta) -> Tensor:\n",
    "    Y = Conv2D_plus(inputs, filters_alpha, 1)\n",
    "    Y = Conv2D_plus(Y, filters_beta, 3)\n",
    "    Y = Add()([Y, inputs])\n",
    "    return Y\n",
    "\n",
    "def ResidualBlock(inputs, num_filters, num_blocks) -> Tensor:\n",
    "    W = Conv2D_plus(inputs, num_filters, 3, stride = 2)\n",
    "    for _ in range(num_blocks):\n",
    "        W = ResidualUnit(W, num_filters // 2, num_filters)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Darknet(inputs, classification = False, num_classes = 10):\n",
    "    Z = Conv2D_plus(inputs, 32, 3)\n",
    "    Z = ResidualBlock(Z, 64, 1)\n",
    "    Z = ResidualBlock(Z, 128, 2)\n",
    "    Z = ResidualBlock(Z, 256, 8)\n",
    "    Z = ResidualBlock(Z, 512, 8)\n",
    "    Z = ResidualBlock(Z, 1024, 4)\n",
    "    if classification:\n",
    "        Z = GlobalAveragePooling2D()(Z)\n",
    "        Z = Dense(num_classes)(Z)\n",
    "        Z = Softmax()(Z)\n",
    "    darknet = Model(inputs=inputs, outputs=Z, name=\"Darknet53\")\n",
    "    return darknet\n",
    "\n",
    "inputs = Input(shape=(452, 602, 3))\n",
    "model = Darknet(inputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i wanna see how the weights are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f903feb0250>\n",
      "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f903feb05e0>\n",
      "(3, 3, 3, 32)\n",
      "<tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x7f8f703f1ee0>\n",
      "(32,)\n",
      "(32,)\n",
      "(32,)\n",
      "(32,)\n",
      "<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x7f903feb0d60>\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(model.get_layer(index = i))\n",
    "    try:\n",
    "        model.get_layer(index = i).get_weights().shape\n",
    "    except:\n",
    "        for l in model.get_layer(index = i).get_weights(): print(l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a dummy input with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 19, 1024)\n"
     ]
    }
   ],
   "source": [
    "def get_test_input():\n",
    "    img_ = tf.io.read_file(\"dog-cycle-car.png\")\n",
    "    img_ = tf.image.decode_png(img_, channels=3)\n",
    "    img_ = tf.image.convert_image_dtype(img_, dtype=tf.float32)\n",
    "    img_ = tf.reshape(img_, (1, 452, 602, 3))\n",
    "    return img_\n",
    "\n",
    "inp = get_test_input()\n",
    "pred = model.predict(inp)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading weights\n",
    "It may be ok not to load any weight if you look at https://arxiv.org/pdf/1811.08883.pdf.\n",
    "\n",
    "Those are the weights obtained after training the darknet53 on the ImageNet Dataset, not the final trained ones. At this point i'm still not sure if they will be trainable in the \"fine training\" phase, we'll see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = Path('/home/andrea/AI/ispr_yolo/weights/darknet_pretrained')\n",
    "fp = weights_path.joinpath('darknet53.weights')\n",
    "weights_array = np.fromfile(fp, dtype = np.float32, offset = 20)\n",
    "weights_num_counter = 0\n",
    "\n",
    "def swapPositions(list, pos1, pos2): \n",
    "    list[pos1], list[pos2] = list[pos2], list[pos1] \n",
    "    return list\n",
    "\n",
    "\n",
    "for idx in range(180):\n",
    "    weights = model.get_layer(index = idx).get_weights()\n",
    "    if len(weights) == 1: # It is a convolutional layer\n",
    "        weights = weights[0]\n",
    "        \n",
    "        #Storing Convolutional weights\n",
    "        conv_weights = weights_array[weights_num_counter + 4*weights.shape[-1] : weights_num_counter + np.prod(weights.shape) + 4*weights.shape[-1] ]\n",
    "        darknet_w_shape = (weights.shape[3], weights.shape[2], weights.shape[0], weights.shape[1])\n",
    "        conv_weights = np.reshape(conv_weights, darknet_w_shape)\n",
    "        conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "        conv_weights = [conv_weights]\n",
    "        \n",
    "        #Storing Batch Norm weights\n",
    "        batch_weights = weights_array[weights_num_counter : weights_num_counter + 4*weights.shape[-1]]\n",
    "        batch_weights_array = []\n",
    "        for array in np.array_split(batch_weights, 4):\n",
    "            batch_weights_array.append(array)\n",
    "        batch_weights_array = swapPositions(batch_weights_array, 1, 0)\n",
    "\n",
    "        weights_num_counter += np.prod(weights.shape) + 4*weights.shape[-1]\n",
    "\n",
    "        model.get_layer(index = idx).set_weights(conv_weights)\n",
    "        model.get_layer(index = idx + 1).set_weights(batch_weights_array)\n",
    "        \n",
    "        \n",
    "save_path = Path('/home/andrea/AI/ispr_yolo/weights')\n",
    "save_path = save_path.joinpath('darknet53.h5')\n",
    "model.save(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
